{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"12OjbRYaw-Iq3ldpQZ9vGkdeZgwT9kNVc","authorship_tag":"ABX9TyNfAlQFrw5mhVL6Z5UL/FoW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Why?\n","\n","\n","reduce the domain gap\n","\n","Artistic Style to Photographic Images"],"metadata":{"id":"KPTv6aBMz-gK"}},{"cell_type":"markdown","source":["Goal -\n"," Reduce the Domain Shift enough to be able to classificy the target domain images with just fine-tunning a model"],"metadata":{"id":"bcBCj76umI4-"}},{"cell_type":"markdown","source":["CycleGAN\n","\n","*   It consists of two generators (G and F) and two discriminators (D_X and\n","    D_Y).\n","*   The goal is to learn mappings between two domains, X → Y and Y → X, without\n","    needing paired examples\n","\n"],"metadata":{"id":"mWFE90Bhuyn_"}},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"zZqX1E995zTF"}},{"cell_type":"code","source":["import os\n","import kagglehub\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image"],"metadata":{"id":"PRANub7r50QN","executionInfo":{"status":"ok","timestamp":1743204487669,"user_tz":300,"elapsed":15,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvhvu_sMNW7d","executionInfo":{"status":"ok","timestamp":1743204487680,"user_tz":300,"elapsed":12,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"4d913db3-cf11-450d-9a5f-c56e06e11d98"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["**Data Processing**"],"metadata":{"id":"75IstDlC3fCH"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"karntiwari/home-office-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Hgwjl3D2qTp","executionInfo":{"status":"ok","timestamp":1743204488512,"user_tz":300,"elapsed":832,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"a9a1a8f1-3bb0-4f48-fff7-77e8ccb09a61"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/karntiwari/home-office-dataset/versions/1\n"]}]},{"cell_type":"code","source":["def walk_through_dir(dir_path):\n","  \"\"\"\n","  Walks through dir_path returning its contents.\n","  Args:\n","    dir_path (str or pathlib.Path): target directory\n","\n","  Returns:\n","    A print out of:\n","      number of subdiretories in dir_path\n","      number of images (files) in each subdirectory\n","      name of each subdirectory\n","  \"\"\"\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n","\n","\n","# walk_through_dir(path)"],"metadata":{"id":"XuFrn0Z469ct","executionInfo":{"status":"ok","timestamp":1743204488518,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"7oSVj08m6sWn","executionInfo":{"status":"ok","timestamp":1743204488524,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"6c12ad6d-5603-4b73-dec9-bd41def12d9b"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/root/.cache/kagglehub/datasets/karntiwari/home-office-dataset/versions/1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#target domain\n","art_path = os.path.join(path, \"OfficeHomeDataset_10072016/Art\")\n","\n","#source domain\n","real_path = os.path.join(path, \"OfficeHomeDataset_10072016/Real World\")"],"metadata":{"id":"fHoidwQq5pru","executionInfo":{"status":"ok","timestamp":1743204488534,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize images to 256x256\n","    transforms.ToTensor(),         # Convert to PyTorch tensor\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n","])"],"metadata":{"id":"qxwlhmtY6Y7F","executionInfo":{"status":"ok","timestamp":1743204488543,"user_tz":300,"elapsed":8,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Load QuickDraw and Real domain datasets\n","quickdraw_dataset = ImageFolder(root=art_path, transform=transform)\n","real_dataset = ImageFolder(root=real_path, transform=transform)\n","\n","quickdraw_loader = torch.utils.data.DataLoader(quickdraw_dataset, batch_size=1, shuffle=True)\n","real_loader = torch.utils.data.DataLoader(real_dataset, batch_size=1, shuffle=True)"],"metadata":{"id":"vtlKvcLG6Pl1","executionInfo":{"status":"ok","timestamp":1743204488554,"user_tz":300,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["len(quickdraw_dataset), len(real_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWvNa3rFqfSW","executionInfo":{"status":"ok","timestamp":1743204488574,"user_tz":300,"elapsed":19,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"805f6feb-1cab-4f95-9f9d-33c545498b4c"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2427, 4357)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["**Models**"],"metadata":{"id":"_865Wchw7jzQ"}},{"cell_type":"markdown","source":["need new definations"],"metadata":{"id":"yZf7Jblf8eL_"}},{"cell_type":"markdown","source":["\n","\n","*   The generator translates images from one domain to another.\n","*   The discriminator classifies whether an image is real or generated.\n","\n"],"metadata":{"id":"WtDAWZFH706P"}},{"cell_type":"markdown","source":["Generator"],"metadata":{"id":"kBOF0LwH8yTY"}},{"cell_type":"code","source":["\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n","            nn.InstanceNorm2d(channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n","            nn.InstanceNorm2d(channels),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)  # Residual connection\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_residual_blocks=9):\n","        super(Generator, self).__init__()\n","        # Initial convolution block\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=3),\n","            nn.InstanceNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        # Downsampling layers\n","        self.down = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n","            nn.InstanceNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n","            nn.InstanceNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        # Residual blocks\n","        self.residuals = nn.Sequential(\n","            *[ResidualBlock(256) for _ in range(num_residual_blocks)]\n","        )\n","        # Upsampling layers\n","        self.up = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.InstanceNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.InstanceNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        # Output layer\n","        self.final = nn.Sequential(\n","            nn.Conv2d(64, out_channels, kernel_size=7, stride=1, padding=3),\n","            nn.Tanh()\n","            # [-1, 1]\n","        )\n","\n","    def forward(self, x):\n","        x = self.initial(x)\n","        x = self.down(x)\n","        x = self.residuals(x)\n","        x = self.up(x)\n","        return self.final(x)"],"metadata":{"id":"md1Pkb4X9Zla","executionInfo":{"status":"ok","timestamp":1743204488576,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels):\n","        super(Discriminator, self).__init__()\n","        # Define PatchGAN layers\n","        self.model = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.InstanceNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.InstanceNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n","            nn.InstanceNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Output single scalar per patch\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"PaONVyTcC7wG","executionInfo":{"status":"ok","timestamp":1743204488584,"user_tz":300,"elapsed":6,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Losses\n","\n","*   Adversarial Loss:\n","*   Cycle-Consistency Loss:\n","\n"],"metadata":{"id":"7T3ovn4OxnbF"}},{"cell_type":"code","source":["adversarial_loss = nn.MSELoss()\n","cycle_loss = nn.L1Loss()"],"metadata":{"id":"WgQmNXttDdkJ","executionInfo":{"status":"ok","timestamp":1743204488588,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["**CycleGAN Training Loop**"],"metadata":{"id":"bYiOTJ0_m6qq"}},{"cell_type":"markdown","source":["The training loop for CycleGAN consists of alternating updates between two generator networks and two discriminator networks.\n"],"metadata":{"id":"JuOatKNrm-NJ"}},{"cell_type":"code","source":["G = Generator(in_channels=3, out_channels=3).to(device)\n","F = Generator(in_channels=3, out_channels=3).to(device)\n","D_A = Discriminator(in_channels=3).to(device)\n","D_B = Discriminator(in_channels=3).to(device)"],"metadata":{"id":"1A_24Ur5sR1A","executionInfo":{"status":"ok","timestamp":1743204607905,"user_tz":300,"elapsed":217,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["optimizer_G = torch.optim.Adam(G.parameters(), lr=0.000025, betas=(0.5, 0.999))\n","optimizer_F = torch.optim.Adam(F.parameters(), lr=0.000025, betas=(0.5, 0.999))\n","optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=0.0005, betas=(0.5, 0.999))\n","optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=0.001, betas=(0.5, 0.999))\n","\n","\n","from torch.optim.lr_scheduler import StepLR\n","scheduler_G = StepLR(optimizer_G, step_size=5, gamma=0.5)  # Reduce by factor of 0.1 every 10 steps\n","scheduler_F = StepLR(optimizer_F, step_size=5, gamma=0.5)\n","scheduler_D_A = StepLR(optimizer_D_A, step_size=5, gamma=0.5)\n","scheduler_D_B = StepLR(optimizer_D_B, step_size=5, gamma=0.5)\n"],"metadata":{"id":"nMetLBhwu8oa","executionInfo":{"status":"ok","timestamp":1743204608468,"user_tz":300,"elapsed":7,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","lambda_cycle = 5\n","#define iterators\n","gen_iter = iter(quickdraw_loader)\n","real_iter = iter(real_loader)\n","\n","\n","\n","from torch.amp import GradScaler, autocast  # Import for mixed precision\n","from itertools import zip_longest  # Use for handling different DataLoader lengths\n","\n","# Initialize GradScaler for mixed precision training\n","scaler = GradScaler()\n","\n","# Training loop\n","for epoch in range(epochs):\n","    # Use zip_longest to handle mismatched DataLoader lengths\n","    for quickdraw_batch, real_batch in zip_longest(quickdraw_loader, real_loader, fillvalue=None):\n","        # Process quickdraw_batch\n","        if quickdraw_batch is not None:\n","            quickdraw_images, _ = quickdraw_batch  # Ignore labels\n","            quickdraw_images = quickdraw_images.to(device)\n","\n","        # Process real_batch\n","        if real_batch is not None:\n","            real_images, _ = real_batch  # Ignore labels\n","            real_images = real_images.to(device)\n","\n","        # Skip iteration if one of the batches is None\n","        if quickdraw_batch is None or real_batch is None:\n","            continue\n","\n","        z_quickdraw = quickdraw_images\n","        z_real = real_images\n","\n","        ### Train Generators ###\n","        with autocast(device_type='cuda'):  # Enable mixed precision\n","            fake_quickdraw = G(z_quickdraw)  # Generate QuickDraw-like images\n","            fake_real = F(z_real)  # Generate Real images\n","\n","            # Cycle consistency\n","            with torch.no_grad():  # Avoid storing intermediate gradients\n","                cycle_quickdraw = F(fake_quickdraw)  # Reverse cycle: A -> B -> A\n","                cycle_real = G(fake_real)  # Reverse cycle: B -> A -> B\n","\n","            # Loss calculation\n","            loss_GAN_g = adversarial_loss(D_B(fake_quickdraw), torch.ones_like(D_B(fake_quickdraw)).to(device))\n","            loss_GAN_f = adversarial_loss(D_A(fake_real), torch.ones_like(D_A(fake_real)).to(device))\n","            loss_cycle = cycle_loss(cycle_quickdraw, z_quickdraw) + cycle_loss(cycle_real, z_real)\n","\n","            loss_G_total = loss_GAN_g + loss_GAN_f + lambda_cycle * loss_cycle\n","\n","        # Backpropagation and optimization with scaled gradients\n","        optimizer_G.zero_grad()\n","        scaler.scale(loss_G_total).backward(retain_graph=True)\n","        scaler.step(optimizer_G)\n","        scaler.update()\n","\n","        # Train Discriminator F\n","        with autocast(device_type='cuda'):  # Enable mixed precision\n","            loss_F_total = loss_GAN_f + lambda_cycle * loss_cycle\n","        optimizer_F.zero_grad()\n","        scaler.scale(loss_F_total).backward()\n","        scaler.step(optimizer_F)\n","        scaler.update()\n","\n","        ### Train Discriminators ###\n","        # D_A\n","        with autocast(device_type='cuda'):\n","            loss_real_A = adversarial_loss(D_A(z_real), torch.ones_like(D_A(z_real)).to(device))\n","            loss_fake_A = adversarial_loss(D_A(fake_real.detach()), torch.zeros_like(D_A(fake_real.detach())).to(device))\n","            loss_D_A = (loss_real_A + loss_fake_A) / 2\n","\n","        optimizer_D_A.zero_grad()\n","        scaler.scale(loss_D_A).backward()\n","        scaler.step(optimizer_D_A)\n","        scaler.update()\n","\n","        # D_B\n","        with autocast(device_type='cuda'):\n","            loss_real_B = adversarial_loss(D_B(z_quickdraw), torch.ones_like(D_B(z_quickdraw)).to(device))\n","            loss_fake_B = adversarial_loss(D_B(fake_quickdraw.detach()), torch.zeros_like(D_B(fake_quickdraw.detach())).to(device))\n","            loss_D_B = (loss_real_B + loss_fake_B) / 2\n","\n","        optimizer_D_B.zero_grad()\n","        scaler.scale(loss_D_B).backward()\n","        scaler.step(optimizer_D_B)\n","        scaler.update()\n","\n","    # Log progress at the end of each epoch\n","    print(f\"Epoch {epoch+1}/{epochs} - Loss_G: {loss_G_total.item():.4f}, \"\n","          f\"Loss_F: {loss_F_total.item():.4f}, Loss_D_A: {loss_D_A.item():.4f}, \"\n","          f\"Loss_D_B: {loss_D_B.item():.4f}\")\n","    # Update learning rate schedulers\n","    scheduler_G.step()\n","    scheduler_F.step()\n","    scheduler_D_A.step()\n","    scheduler_D_B.step()\n","\n","\n","    # Clear unused memory\n","    torch.cuda.empty_cache()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiGL5HMN8n2f","outputId":"644a902b-b3c6-4e50-d0fc-bce0d8869750","executionInfo":{"status":"ok","timestamp":1743208853274,"user_tz":300,"elapsed":4241349,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Loss_G: 6.5575, Loss_F: 6.3069, Loss_D_A: 0.0754, Loss_D_B: 0.2460\n","Epoch 2/10 - Loss_G: 7.1973, Loss_F: 6.9931, Loss_D_A: 0.2243, Loss_D_B: 0.3103\n","Epoch 3/10 - Loss_G: 8.0973, Loss_F: 7.7318, Loss_D_A: 0.1714, Loss_D_B: 0.2557\n","Epoch 4/10 - Loss_G: 8.9121, Loss_F: 8.6646, Loss_D_A: 0.2167, Loss_D_B: 0.2153\n","Epoch 5/10 - Loss_G: 8.0969, Loss_F: 7.7281, Loss_D_A: 0.0644, Loss_D_B: 0.2283\n","Epoch 6/10 - Loss_G: 10.6232, Loss_F: 10.4350, Loss_D_A: 0.0238, Loss_D_B: 0.2547\n","Epoch 7/10 - Loss_G: 11.3655, Loss_F: 10.8175, Loss_D_A: 0.0340, Loss_D_B: 0.3024\n","Epoch 8/10 - Loss_G: 9.3688, Loss_F: 8.8938, Loss_D_A: 0.0866, Loss_D_B: 0.2154\n","Epoch 9/10 - Loss_G: 9.2278, Loss_F: 8.6637, Loss_D_A: 0.0244, Loss_D_B: 0.3427\n","Epoch 10/10 - Loss_G: 7.6658, Loss_F: 7.4510, Loss_D_A: 0.0197, Loss_D_B: 0.2439\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# # Save the models to your Google Drive\n","# torch.save(G.state_dict(), '/content/drive/My Drive/cycDAN/G.pth')\n","# torch.save(F.state_dict(), '/content/drive/My Drive/cycDAN/F.pth')\n","# torch.save(D_A.state_dict(), '/content/drive/My Drive/cycDAN/D_A.pth')\n","# torch.save(D_B.state_dict(), '/content/drive/My Drive/cycDAN/D_B.pth')\n","\n"],"metadata":{"id":"02MH8DE-QT0i","executionInfo":{"status":"aborted","timestamp":1743204514353,"user_tz":300,"elapsed":24,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["visialization"],"metadata":{"id":"ow_IA4xxNOZh"}},{"cell_type":"markdown","source":["**Translate Source Domain Images**"],"metadata":{"id":"-zd7v9nuTA3A"}},{"cell_type":"markdown","source":["  Utilize the trained generators to convert all source domain images into the style of the target domain. The resulting adapted dataset will then serve as a foundation for downstream tasks such as classification."],"metadata":{"id":"NilflNUbTYvQ"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","translated_images = []  # List to hold translated images\n","labels = []  # List to hold corresponding labels (if needed)\n","\n","G.eval()  # Set the generator to evaluation mode\n","for batch in quickdraw_loader:  # Loop through source DataLoader\n","    images, batch_labels = batch  # Get images and labels from the batch\n","    images = images.to(device)  # Move images to the CUDA device\n","\n","    with torch.no_grad():  # No gradients needed for evaluation\n","        translated_batch = G(images)  # Generate translated images\n","\n","    translated_images.append(translated_batch.cpu())  # Move translated images to CPU and store\n","    labels.append(batch_labels)  # Keep labels if you need them later\n","\n","# Combine all translated images and labels\n","translated_images = torch.cat(translated_images, dim=0)  # Combine into a single tensor\n","labels = torch.cat(labels, dim=0)  # Combine labels, if applicable\n"],"metadata":{"id":"64CaMYcsTnjk","executionInfo":{"status":"ok","timestamp":1743208939730,"user_tz":300,"elapsed":80335,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["class TranslatedDataset(Dataset):\n","    def __init__(self, images, labels=None):\n","        self.images = images\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.images[idx], self.labels[idx]\n","        else:\n","            return self.images[idx]\n","\n","# Initialize the custom dataset\n","translated_dataset = TranslatedDataset(translated_images, labels)\n","translated_source_loader = DataLoader(translated_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"X-bau9_9Vpim","executionInfo":{"status":"ok","timestamp":1743208945980,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["**Fine-Tune for Classification**"],"metadata":{"id":"DtryQvdCTE0J"}},{"cell_type":"markdown","source":["Once the source domain images have been adapted to match the target domain's distribution, fine-tune a classification model using the new source domain images"],"metadata":{"id":"9suvYa5kTk5b"}},{"cell_type":"code","source":["# Example: Training a classifier\n","\n","num_classes = 345\n","\n","classifier = torchvision.models.resnet18(pretrained=True)\n","classifier.fc = nn.Linear(classifier.fc.in_features, num_classes)  # Replace final layer for your task\n","classifier.to(device)\n","\n","optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 10\n","\n","# Train on translated source images\n","for epoch in range(num_epochs):\n","    for images, labels in translated_source_loader:  # Adapted dataset loader\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = classifier(images)\n","        loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"],"metadata":{"id":"HwKxotERTskg","executionInfo":{"status":"ok","timestamp":1743209109745,"user_tz":300,"elapsed":87401,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ed2c1b7-43d5-4934-a90b-8fb675446a54"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 3.6808\n","Epoch [2/10], Loss: 2.6575\n","Epoch [3/10], Loss: 1.6770\n","Epoch [4/10], Loss: 1.1055\n","Epoch [5/10], Loss: 0.5429\n","Epoch [6/10], Loss: 0.1904\n","Epoch [7/10], Loss: 0.1130\n","Epoch [8/10], Loss: 0.0803\n","Epoch [9/10], Loss: 0.0517\n","Epoch [10/10], Loss: 0.0544\n"]}]},{"cell_type":"markdown","source":["**Conclusion**"],"metadata":{"id":"mkp_gWDvXQRQ"}},{"cell_type":"markdown","source":["The steady decline in training loss indicates strong model convergence with the adapted dataset loader, and with proper validation, this workflow can achieve excellent generalization on the target domain. By tackling domain shift creatively and efficiently, you're enhancing model robustness for real-world computer vision applications."],"metadata":{"id":"JA7gFF4AXbyg"}}]}